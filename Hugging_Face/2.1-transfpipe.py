#pip install  transformers accelerate
#pip install langchain langchain-community langchain-core
#might be needed
    #pip install hf_xet

#This code bypasses Hugging Faceâ€™s API and runs the model locally (may require GPU).
#Instead of HuggingFaceHub, Running by loading the model locally
#no api keys required here

from transformers import pipeline
#from langchain.llms import HuggingFacePipeline <--deprecated
from langchain_community.llms import HuggingFacePipeline

pipe = pipeline("text2text-generation", model="google/flan-t5-large")
# response = pipe("Explain the concept of black holes in simple terms.")
# print('first output: ',response[0]['generated_text'])

questions = [
    "Explain the concept of black holes in simple terms.",
    "What are the main causes of climate change, and how can we address them?",
    "Provide a brief overview of the history of artificial intelligence."
]

for question in questions:
    response = pipe(question)
    print(f"Q: {question}\nA: {response}\n")

for question in questions:
    response = pipe(question)
    print(response[0]['generated_text'])
