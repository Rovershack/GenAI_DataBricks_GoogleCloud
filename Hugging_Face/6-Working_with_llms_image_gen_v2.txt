THEORY>>>>

#a step-by-step breakdown of how noising and denoising works when generating an image
 (e.g., a tiger) using a diffusion model (like Stable Diffusion, DDPM etc.)

#Generate a high-quality image of a tiger from pure noise, using a diffusion model.

#1.Start with a Clean Image (Training Phase)
This step is for training the model (not generation yet). Let’s assume we already have a clean image of a tiger.

Image: Tiger (high-resolution, clean)

Notation: x₀ = original image

2. Noising Process (Forward Diffusion)
The clean image is gradually corrupted by adding Gaussian noise step-by-step over T timesteps.
At each step t, noise is added:
Where:

xₜ = noised image at timestep t
αₜ = noise schedule parameter
ε = random Gaussian noise

Repeat this process until the image is pure noise at timestep T.

x₀: [Tiger] → x₁ → x₂ → ... → xT: [Pure noise]

3. Train Model to Reverse Noise (Learn ε)
The model is trained to predict the noise ε added at each step.

ε_θ: model's predicted noise
Goal: learn how to "undo" the noise at each step

4. Sampling (Image Generation via Denoising)
Now, to generate a new image of a tiger, we reverse the process.

Step-by-Step Denoising:
Start from pure Gaussian noise x_T

For each t from T to 1:
Predict noise ε_θ(xₜ, t)
Subtract it to get xₜ₋₁

σₜ · z: optional noise for stochastic sampling
Each step brings image closer to realism

5. Final Output
At t=0, the final image x₀ is a realistic image of a tiger generated from random noise.

==============================================
Step-by-Step: Noising a Tiger Image

1. Original Image (x₀)
We'll start with a clear image of a tiger.

2. Progressive Noising
In the forward diffusion process, we gradually add Gaussian noise to the image over multiple timesteps. Here's how the image evolves:

Timestep x₁ (Slight Noise)
The image has a slight graininess; the tiger is still clearly visible.

Timestep x₅ (Moderate Noise)
The image appears more blurred; details start to fade.

Timestep x₁₀ (Heavy Noise)
The tiger is barely recognizable; noise dominates the image.

Timestep x_T (Pure Noise)
The image is now pure noise; no discernible features remain.

3. Summary
This process demonstrates how a diffusion model adds noise to an image step-by-step. During training, the model learns to reverse this process, 
effectively denoising the image to recover the original content.

Step-by-Step: Denoising (Tiger Image Generation)
After the model is trained, we reverse the noising steps to generate an image from pure noise. 

Step 0: Start with Pure Noise (x_T)
This is the starting point: a random noise image (no tiger yet!).

Step-by-Step Denoising
At each step t, the model predicts the noise added, then subtracts it from the current image

Timestep x₁₀ → x₉
Some vague shapes and colors start to appear — like fur texture or background hints.

Timestep x₅ → x₄
The image becomes sharper. You can start to see outlines of the tiger’s face or body.

Timestep x₂ → x₁
The stripes, eyes, and fur become clear.

Final Output: x₀ (Generated Tiger Image)
Now we have a full, photo-realistic image of a tiger generated from noise.

===========================
#CODE>>
#code that simulates noising and denoising using PyTorch

--Gaussian noise for the forward diffusion
--A dummy denoising function (to simulate reverse process)
--A sample image (you can replace with an actual tiger image)

#Dependencies
pip install torch torchvision matplotlib

import torch
import torchvision.transforms as T
import torchvision
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import requests
from io import BytesIO

# Download sample tiger image
url = "https://images.pexels.com/photos/145939/pexels-photo-145939.jpeg"
response = requests.get(url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# Resize & transform to tensor
transform = T.Compose([
    T.Resize((64, 64)),
    T.ToTensor()
])
img_tensor = transform(img)

# Show original image
def show_image(tensor, title=""):
    img = tensor.permute(1, 2, 0).numpy()
    plt.imshow(img)
    plt.title(title)
    plt.axis("off")
    plt.show()

show_image(img_tensor, title="Original Image")

# Noising function
def add_noise(x, noise_level):
    noise = torch.randn_like(x) * noise_level
    return x + noise

# Simulate progressive noising
timesteps = [0.1, 0.3, 0.5, 1.0]  # different levels of Gaussian noise
noised_images = [add_noise(img_tensor, t) for t in timesteps]

# Show noised images
for i, (t, img) in enumerate(zip(timesteps, noised_images)):
    show_image(torch.clamp(img, 0, 1), title=f"Noised @ level {t}")

# Simulate dummy denoising (not learned — just for illustration)
def denoise(x, noise_level):
    return torch.clamp(x - (torch.randn_like(x) * noise_level * 0.8), 0, 1)

# Simulate reverse steps
denoised_images = []
current = noised_images[-1]
for step in [0.5, 0.3, 0.1]:
    current = denoise(current, step)
    denoised_images.append(current)

# Show denoised steps
for i, img in enumerate(denoised_images):
    show_image(img, title=f"Denoised Step {i+1}")

===================================

#from a simulation (like our dummy denoising function) to actual image generation using a trained model 
(like a U-Net in a diffusion pipeline).

To truly generate a new image (like a tiger) from noise, we need:

--What’s Required
A trained diffusion model (e.g., DDPM, Stable Diffusion)
A sampling loop: goes from x_T (pure noise) to x₀ (final image)
(Optional) A text prompt if it’s a text-to-image model

--Using real model
#using diffusers (Hugging Face library)

#dependencies
pip install diffusers transformers accelerate

from diffusers import StableDiffusionPipeline
import torch
import matplotlib.pyplot as plt

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")

# Prompt to generate a tiger
prompt = "A majestic tiger standing in a jungle, photorealistic"

image = pipe(prompt).images[0]
image.show()

--------------------------------
#to train your own diffusion model on tiger images

#building a basic DDPM (Denoising Diffusion Probabilistic Model) from scratch using PyTorch.

#dependencies
pip install torch torchvision matplotlib tqdm

#prepare a dataset of tiger images:

Size: 64x64 or 128x128 to start small
Format: JPG/PNG
Folder structure: 
data/
  tigers/
    001.jpg
    002.jpg
    ...

#Step 1: Dataset & Preprocessing
from torchvision import transforms, datasets
from torch.utils.data import DataLoader

transform = transforms.Compose([
    transforms.Resize(64),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
])

dataset = datasets.ImageFolder("data", transform=transform)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

#Step 2: Define the U-Net (Core of DDPM)
import torch.nn as nn
import torch

class SimpleUNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(4, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, 3, padding=1)
        )

    def forward(self, x, t):
        # Concatenate time embedding as extra channel
        time_emb = torch.ones_like(x[:, :1, :, :]) * t.float().view(-1, 1, 1, 1) / 1000
        x = torch.cat([x, time_emb], dim=1)
        return self.net(x)

#Step 3: Forward Diffusion Process (Add Noise)
def noise_schedule(t, beta_start=1e-4, beta_end=0.02, T=1000):
    return beta_start + (beta_end - beta_start) * (t / T)

def add_noise(x, t):
    beta = noise_schedule(t)
    noise = torch.randn_like(x)
    alpha = 1.0 - beta
    return torch.sqrt(alpha) * x + torch.sqrt(beta) * noise, noise

#Step 4: Training Loop
model = SimpleUNet().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
T = 1000

for epoch in range(10):
    for x, _ in dataloader:
        x = x.cuda()
        t = torch.randint(0, T, (x.size(0),), device=x.device)
        noised_x, noise = add_noise(x, t)
        predicted_noise = model(noised_x, t)
        
        loss = nn.MSELoss()(predicted_noise, noise)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch} Loss: {loss.item()}")

#Step 5: Sampling (Generate New Images)
def sample(model, shape=(1, 3, 64, 64), T=1000):
    model.eval()
    x = torch.randn(shape).cuda()

    for t in reversed(range(T)):
        t_tensor = torch.full((shape[0],), t, device=x.device)
        predicted_noise = model(x, t_tensor)
        beta = noise_schedule(t)
        alpha = 1.0 - beta
        x = (x - torch.sqrt(beta) * predicted_noise) / torch.sqrt(alpha)

    return torch.clamp(x, 0, 1)

# Generate and show a tiger
with torch.no_grad():
    generated = sample(model, shape=(1, 3, 64, 64))
    import matplotlib.pyplot as plt
    plt.imshow(generated[0].permute(1, 2, 0).cpu())
    plt.axis("off")
    plt.show()

------------------------------------
#Building DDIM (Denoising Diffusion Implicit Models) instead of DDPM, we're aiming for faster sampling without needing 
to retrain the entire diffusion model.

| Feature  | DDPM                                | DDIM                                |
| -------- | ----------------------------------- | ----------------------------------- |
| Sampling | Stochastic (random noise each step) | Deterministic (no added randomness) |
| Speed    | Slow (1000 steps typical)           | Fast (can use 10–50 steps!)         |
| Accuracy | Very high                           | High but efficient                  |
| Training | Same model can be reused            | No retraining needed                |

--For DDIM: We'll modify the sampling process only. You reuse the same trained U-Net.

1.--Assume you’ve already trained your model like before.
--Use Trained U-Net (From DDPM)

model.eval()

2. --Define DDIM Parameters
import numpy as np

def make_ddim_schedule(T, ddim_steps):
    # Evenly spaced steps from T down to 0
    return np.linspace(0, T - 1, ddim_steps, dtype=int)[::-1]

3. --DDIM Sampling Function (deterministic sampling function using DDIM):
@torch.no_grad()
def ddim_sample(model, shape=(1, 3, 64, 64), T=1000, ddim_steps=50):
    x = torch.randn(shape).cuda()
    timesteps = make_ddim_schedule(T, ddim_steps)

    alphas = [1 - noise_schedule(t) for t in timesteps]
    alphas_bar = np.cumprod(alphas)

    for i in range(len(timesteps) - 1):
        t = torch.full((shape[0],), timesteps[i], device=x.device)
        t_next = timesteps[i+1]

        # Predict noise
        pred_noise = model(x, t)

        # Compute current alpha_bar and next alpha_bar
        alpha_bar = torch.tensor(alphas_bar[i], device=x.device)
        alpha_bar_next = torch.tensor(alphas_bar[i+1], device=x.device)

        # Predict x0 (original image estimate)
        x0_pred = (x - torch.sqrt(1 - alpha_bar) * pred_noise) / torch.sqrt(alpha_bar)

        # DDIM deterministic update
        x = torch.sqrt(alpha_bar_next) * x0_pred + torch.sqrt(1 - alpha_bar_next) * pred_noise

    return torch.clamp(x, 0, 1)

4. Generate Tiger Image
generated = ddim_sample(model, shape=(1, 3, 64, 64), ddim_steps=50)

import matplotlib.pyplot as plt
plt.imshow(generated[0].permute(1, 2, 0).cpu())
plt.axis("off")
plt.show()

What changed:
| Thing           | Before (DDPM)      | Now (DDIM)                    |
| --------------- | ------------------ | ----------------------------- |
| Sampling method | Random + iterative | Deterministic + fast          |
| Sampling steps  | \~1000             | \~50 (or even 25!)            |
| Model used      | Same U-Net         | Same                          |
| Output          | New tiger image    | New tiger image (much faster) |

--------------------------------------------------


